{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kZHKLRDkm-ne"
      },
      "outputs": [],
      "source": [
        "##Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(filename='whaaaat.log', level=logging.DEBUG)\n",
        "logging.info('Ready to log.')\n",
        "\n",
        "def log_print(text, level='info'):\n",
        "    print(text)\n",
        "    if level=='info':\n",
        "        logging.info(text)\n",
        "    elif level=='error':\n",
        "        logging.error(text)\n",
        "    else:\n",
        "        logging.debug(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import eli5\n",
        "from sklearn_crfsuite import metrics, scorers, CRF\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin \n",
        "import pickle\n",
        "from datetime import datetime as dt\n",
        "import spacy\n",
        "import json\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "log_print('Imports successful.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyrpJnpfnCmP"
      },
      "outputs": [],
      "source": [
        "##Inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_frames(path, k_fold=True):\n",
        "    data = {i:pd.read_csv(path+f'{i}.csv', index_col=['sentence_id','token_id']).fillna(value={'token': 'NA'}) for i in ('train','validation','test')}\n",
        "\n",
        "    if k_fold:\n",
        "        offset = data['train'].index.max(0)[0]+1\n",
        "        df = data['validation']\n",
        "        df.index = df.index.set_levels(df.index.levels[0]+offset, level=0)\n",
        "        data['train'] = pd.concat([data['train'],data['validation']], verify_integrity=True)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    path = '../data/' if 'src' in os.getcwd() else './data/'\n",
        "    data_conll = get_frames(path+'conll/')\n",
        "    data_wnut = get_frames(path+'wnut/')\n",
        "\n",
        "    logging.info('Frames loaded from file.')\n",
        "except Exception as e:\n",
        "    logging.error(f'Failed: {e}')\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "tag_map = {\n",
        "    'B-product':'B-MISC',\n",
        "    'I-product':'I-MISC',\n",
        "    'B-creative-work':'B-MISC',\n",
        "    'I-creative-work':'I-MISC',\n",
        "    'B-corporation':'B-ORG',\n",
        "    'I-corporation':'I-ORG',\n",
        "    'B-group':'B-MISC',\n",
        "    'I-group':'I-MISC',\n",
        "    'B-person':'B-PER',\n",
        "    'I-person':'I-PER',\n",
        "    'B-location':'B-LOC',\n",
        "    'I-location':'I-LOC',\n",
        "    'O':'O'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    for frame in data_wnut.values():\n",
        "        frame['tag'] = frame['tag'].map(tag_map)\n",
        "\n",
        "    wnut_sort = sorted(data_conll['train']['tag'].unique(), key=lambda x: (x[1:],x[0]))\n",
        "    conll_sort = sorted(data_wnut['train']['tag'].unique(), key=lambda x: (x[1:],x[0]))\n",
        "\n",
        "    assert all([i==j for i,j in zip(wnut_sort,conll_sort)]), 'Mismatched tags between CoNLL and WNUT data.'\n",
        "\n",
        "    log_print(f'Tag conversion successful. Tags: {str(wnut_sort)}')\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f'Failed: {e}')\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the self-annotated tweets\n",
        "tweets = pd.read_csv(path+f'twitter/test.csv', index_col=['sentence_id','token_id']).fillna(value={'token': 'NA'})\n",
        "tweet_map = {\n",
        "    'B-EVENT':'B-MISC',\n",
        "    'I-EVENT':'I-MISC',\n",
        "    'B-NORP':'B-MISC',\n",
        "    'I-NORP':'I-MISC',\n",
        "    'B-WORK_OF_ART':'B-MISC',\n",
        "    'I-WORK_OF_ART':'I-MISC',\n",
        "    'B-PRODUCT':'B-MISC',\n",
        "    'I-PRODUCT':'I-MISC',\n",
        "    'B-FAC':'B-MISC',\n",
        "    'I-FAC':'I-MISC',\n",
        "    'B-PERSON':'B-PER',\n",
        "    'I-PERSON':'I-PER',\n",
        "    'B-GPE':'B-LOC',\n",
        "    'I-GPE':'I-LOC',\n",
        "    'B-LOC':'B-LOC',\n",
        "    'I-LOC':'I-LOC',\n",
        "    'B-ORG':'B-ORG',\n",
        "    'I-ORG':'I-ORG',\n",
        "    'O':'O'\n",
        "}\n",
        "\n",
        "tweets['tag'] = tweets['tag'].map(tweet_map)\n",
        "\n",
        "tweet_tags = sorted(tweets['tag'].unique(), key=lambda x: (x[1:],x[0]))\n",
        "log_print(f'Testing tweets ready. Tags: {str(tweet_tags)}')\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zDRRM_CPrgkN"
      },
      "outputs": [],
      "source": [
        "##Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p2CsZO-CuLpE"
      },
      "outputs": [],
      "source": [
        "Coming up with good features for our model to assign weights to is critical. We've used a number of orthographic features, prefixes, suffixes, POS tags and lemmas. The tag transition is also modelled under the hood by crfsuite. The context window is of radius 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sent2():\n",
        "    '''Takes an array of sentences and their tags'''\n",
        "\n",
        "    def __init__(self,attributes=None):\n",
        "        self.attrs = attributes\n",
        "\n",
        "    def __get_features(self, word, prefix):\n",
        "        '''get features dictionary for a single token'''\n",
        "\n",
        "        try:\n",
        "\n",
        "            features = {\n",
        "                f'{prefix}{attr}': word[attr] for attr in self.attrs\n",
        "            }\n",
        "\n",
        "            features.update({f'{prefix}bias': 1.0,\n",
        "            f'{prefix}prefix2': word['token'][:2],\n",
        "            f'{prefix}prefix3': word['token'][:2],\n",
        "            f'{prefix}suffix2': word['token'][-2:],\n",
        "            f'{prefix}suffix3': word['token'][-3:],\n",
        "            })\n",
        "\n",
        "        except TypeError:\n",
        "            key = 'BOS' if prefix == '-1' else 'EOS'\n",
        "            features = {key: True}\n",
        "        return features\n",
        "\n",
        "    def __word2features(self, sent, token_id):\n",
        "        '''get features dictionary over the context window'''\n",
        "\n",
        "        # get rows of context window\n",
        "        current = sent.iloc[token_id]\n",
        "        left = sent.iloc[token_id - 1] if token_id else None\n",
        "        try:\n",
        "            right = sent.iloc[token_id + 1]\n",
        "        except:\n",
        "            right = None\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        # add features from all tokens in context window\n",
        "        for row, prefix in zip((current, left, right), ('', '-1', '+1')):\n",
        "            features.update(self.__get_features(row, prefix))\n",
        "\n",
        "        features.pop('-1bias', None)\n",
        "        features.pop('+1bias', None)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def features(self, sent):\n",
        "        '''convert a sentence to a list of context window feature vectors'''\n",
        "        return [self.__word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "    def tokens(self, sent):\n",
        "        '''get the sequence of string tokens for a sentence'''\n",
        "        return ' '.join([row['token'] for _, row in sent.iterrows()])\n",
        "\n",
        "    def labels(self,sent):\n",
        "        '''get the sequence of target tags for a sentence'''\n",
        "        return [row['tag'] for _, row in sent.iterrows()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uxXwsghjvJgV"
      },
      "outputs": [],
      "source": [
        "Sample sentences from our dataframes and create the input and output pairs for training/testing. An input is a list of feature vectors, one for each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentences(frame, p=1, name='unknown'):\n",
        "\n",
        "    frame = frame.drop(columns='cluster')\n",
        "\n",
        "    if p<1:\n",
        "        sample_idx = np.random.choice(range(len(frame.groupby(level='sentence_id'))),\n",
        "                                size=int(p*len(frame.groupby(level='sentence_id'))),\n",
        "                                replace=False)\n",
        "        frame = frame.loc[sample_idx,:]\n",
        "\n",
        "    att = list(frame.columns.values)\n",
        "    att.remove('tag')\n",
        "    # print('Using token features:\\n',att)\n",
        "\n",
        "    x = frame.drop(columns='tag'\n",
        "                    ).groupby(level='sentence_id'\n",
        "                    ).apply(lambda sent: Sent2(att).features(sent))\n",
        "    y = frame.groupby(level='sentence_id'\n",
        "                    ).apply(lambda sent: Sent2().labels(sent))\n",
        "    z = frame.groupby(level='sentence_id'\n",
        "                    ).apply(lambda sent: Sent2().tokens(sent))\n",
        "\n",
        "\n",
        "    # return pd.DataFrame(data={'x': x, 'y': y, 'tokens': z, 'dist': dist})\n",
        "    return pd.DataFrame(data={'x': x, 'y': y, 'z': z, 'dist': [name]*len(x)})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = 1\n",
        "\n",
        "try:\n",
        "    log_print('Getting sentences from multi-index dataframes...')\n",
        "    dists = {}\n",
        "    dists['tweets'] = get_sentences(tweets, 1, 'tweet')\n",
        "    dists['conll'] = get_sentences(data_conll['train'], p, 'conll')\n",
        "    dists['wnut'] = get_sentences(data_wnut['train'], p, 'wnut')\n",
        "    dists['mixed'] = pd.concat([train_conll,train_wnut]).reset_index(drop=True).sample(frac=1)\n",
        "    log_print(f'Conversion to sentences successful.')\n",
        "except Exception as e:\n",
        "    logging.error(f'Failed: {e}')\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GajuLtYe4aCx"
      },
      "outputs": [],
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zl2-HRyWvdty"
      },
      "outputs": [],
      "source": [
        "The 'O' is not of interest to us; we are more interested in the other tags.\n",
        "Let's evaluate models based on a flat f1 score over the other tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = list(data_conll['train']['tag'].unique())\n",
        "labels.remove('O')\n",
        "\n",
        "flat_f1 = make_scorer(metrics.flat_f1_score, average='weighted', labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "get_model = 'gs' # gs, fit\n",
        "dist = 'mixed'\n",
        "date = '27_11_19'\n",
        "model_name = f'./models/crf_{date}_{dist}.pickle'\n",
        "train = dists[dist]\n",
        "\n",
        "try:\n",
        "    if get_model == 'gs':\n",
        "        iters = 20\n",
        "        cv = 3\n",
        "        params_space = {\n",
        "        'c1': scipy.stats.expon(scale=1),\n",
        "        'c2': scipy.stats.expon(scale=0.02),\n",
        "        }\n",
        "\n",
        "        logging.info(f'Fitting {iters*cv} models...')\n",
        "        rs = RandomizedSearchCV(CRF(all_possible_transitions=True), params_space, n_iter=iters, cv=cv, scoring=flat_f1, n_jobs=10, verbose=1)\n",
        "        rs.fit(train['x'], train['y'])\n",
        "        clf = rs.best_estimator_\n",
        "\n",
        "        log_print(f'Fit successful. Best params {rs.best_params_}')\n",
        "    elif get_model == 'fit':\n",
        "        logging.info(f'Fitting full model...')\n",
        "        clf = CRF(all_possible_transitions=True, c1=0.0798, c2=0.0523)\n",
        "        clf.fit(train['x'], train['y'])\n",
        "        log_print(f'Fit successful.')\n",
        "    else:\n",
        "        raise Exception('Unknown operation.')\n",
        "except Exception as e:\n",
        "    logging.error(f'Failed: {e}')\n",
        "    raise e\n",
        "finally:\n",
        "    try:\n",
        "        with open(model_name, 'wb') as f:\n",
        "            pickle.dump(clf,f)\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UIZh7ucB5FwC"
      },
      "outputs": [],
      "source": [
        "##Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Loaded pre-trained model ./models/crf_25_11_19_conll.pickle.\nc1:0.0798, c2:0.0523\ncrf_25_11_19_conll flat F1 score: 0.36757416700651074\n              precision    recall  f1-score   support\n\n       B-LOC      0.380     0.576     0.458        66\n       I-LOC      0.074     0.250     0.114         8\n      B-MISC      0.392     0.326     0.356        95\n      I-MISC      0.125     0.148     0.135        61\n       B-ORG      0.299     0.386     0.337        83\n       I-ORG      0.138     0.355     0.198        31\n       B-PER      0.465     0.488     0.476       162\n       I-PER      0.333     0.491     0.397        53\n\n   micro avg      0.320     0.408     0.358       559\n   macro avg      0.276     0.377     0.309       559\nweighted avg      0.345     0.408     0.368       559\n\nLoaded pre-trained model ./models/crf_25_11_19_wnut.pickle.\nc1:0.019877889915629585, c2:0.005732039148371734\ncrf_25_11_19_wnut flat F1 score: 0.3707677922604899\n              precision    recall  f1-score   support\n\n       B-LOC      0.477     0.470     0.473        66\n       I-LOC      0.138     0.500     0.216         8\n      B-MISC      0.220     0.116     0.152        95\n      I-MISC      0.200     0.164     0.180        61\n       B-ORG      0.875     0.169     0.283        83\n       I-ORG      0.000     0.000     0.000        31\n       B-PER      0.664     0.562     0.609       162\n       I-PER      0.647     0.415     0.506        53\n\n   micro avg      0.480     0.327     0.389       559\n   macro avg      0.403     0.299     0.302       559\nweighted avg      0.501     0.327     0.371       559\n\nLoaded pre-trained model ./models/crf_27_11_19_mixed.pickle.\nc1:0.0656, c2:0.025\ncrf_27_11_19_mixed flat F1 score: 0.49314644866355584\n              precision    recall  f1-score   support\n\n       B-LOC      0.564     0.667     0.611        66\n       I-LOC      0.158     0.375     0.222         8\n      B-MISC      0.492     0.316     0.385        95\n      I-MISC      0.326     0.230     0.269        61\n       B-ORG      0.457     0.386     0.418        83\n       I-ORG      0.268     0.355     0.306        31\n       B-PER      0.705     0.605     0.651       162\n       I-PER      0.651     0.528     0.583        53\n\n   micro avg      0.526     0.465     0.494       559\n   macro avg      0.453     0.433     0.431       559\nweighted avg      0.537     0.465     0.493       559\n\n"
        }
      ],
      "source": [
        "models = ['crf_25_11_19_conll','crf_25_11_19_wnut','crf_27_11_19_mixed']\n",
        "\n",
        "for name in models:\n",
        "    model_name = f'./models/{name}.pickle'\n",
        "\n",
        "    with open(model_name, 'rb') as f:\n",
        "        clf = pickle.load(f)\n",
        "        log_print(f'Loaded pre-trained model {model_name}.')\n",
        "\n",
        "    log_print(f'c1:{clf.c1}, c2:{clf.c2}')\n",
        "    \n",
        "    y_pred = clf.predict(test_tweet['x'])\n",
        "\n",
        "    flat_f1 = metrics.flat_f1_score(test_tweet['y'], y_pred,\n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "    log_print(f'{name} flat F1 score: '+str(flat_f1))\n",
        "\n",
        "    sorted_labels = sorted(labels, key = lambda x: (x[1:],x[0]))\n",
        "\n",
        "    print(metrics.flat_classification_report(\n",
        "        test_tweet['y'], y_pred, labels=sorted_labels, digits=3\n",
        "    ))\n",
        "\n",
        "\n",
        "    # if os.name == 'nt':\n",
        "    #     eli5.show_weights(clf, top=10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "if get_model == 'gs' and os.name == 'nt':\n",
        "    _x = [s['c1'] for s in rs.cv_results_['params']]\n",
        "    _y = [s['c2'] for s in rs.cv_results_['params']]\n",
        "    _c = [round(s,2) for s in rs.cv_results_['mean_test_score']]\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(12, 12)\n",
        "    ax = plt.gca()\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('C1')\n",
        "    ax.set_ylabel('C2')\n",
        "    ax.set_title(\"Randomized Hyperparameter Search Sampled CV Results (min={:0.3}, max={:0.3})\".format(\n",
        "        min(_c), max(_c)\n",
        "    ))\n",
        "\n",
        "    sns.scatterplot(x=_x, y=_y, hue=_c, size=_c, ax=ax)\n",
        "\n",
        "    # ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
        "\n",
        "    print(f'Dark blue => {round(min(_c),3)}, dark red => {round(max(_c),3)}')\n",
        "\n",
        "    logging.info('Evaluation successful.')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}