{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "outputs": [],
      "source": [
        "##Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "outputs": [],
      "source": [
        "Set up the logging and import the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(filename='notebook.log', level=logging.DEBUG)\n",
        "logging.info('Ready to log.')\n",
        "\n",
        "def log_print(text, level='info'):\n",
        "    print(text)\n",
        "    if level=='info':\n",
        "        logging.info(text)\n",
        "    elif level=='error':\n",
        "        logging.error(text)\n",
        "    else:\n",
        "        logging.debug(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import eli5\n",
        "from sklearn_crfsuite import metrics, scorers, CRF, utils\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, confusion_matrix\n",
        "from sklearn.base import BaseEstimator, TransformerMixin \n",
        "import pickle\n",
        "from datetime import datetime as dt\n",
        "import spacy\n",
        "import json\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.join(os.getcwd(),'../NER-evaluation'))\n",
        "\n",
        "log_print('Imports successful.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyrpJnpfnCmP"
      },
      "outputs": [],
      "source": [
        "##Inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_frames(path, k_fold=True):\n",
        "    ''' Load the multi-index dataframes from file, join training and validation frames if we are using k-fold CV.\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    data : dict\n",
        "     Training, validation (if not k-fold CV), and test frames.\n",
        "    '''\n",
        "\n",
        "    data = {i:pd.read_csv(path+f'{i}.csv', index_col=['sentence_id','token_id']).fillna(value={'token': 'NA'}) for i in ('train','validation','test')}\n",
        "\n",
        "    if k_fold:\n",
        "        offset = data['train'].index.max(0)[0]+1\n",
        "        df = data['validation']\n",
        "        df.index = df.index.set_levels(df.index.levels[0]+offset, level=0)\n",
        "        data['train'] = pd.concat([data['train'],data['validation']], verify_integrity=True)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    path = '../data/' if 'src' in os.getcwd() else './data/'\n",
        "    data_conll = get_frames(path+'conll/')\n",
        "    data_wnut = get_frames(path+'wnut/')\n",
        "\n",
        "    logging.info('Frames loaded from file.')\n",
        "except Exception as e:\n",
        "    logging.error(f'Failed: {e}')\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "tag_map = {\n",
        "    'B-product':'B-MISC',\n",
        "    'I-product':'I-MISC',\n",
        "    'B-creative-work':'B-MISC',\n",
        "    'I-creative-work':'I-MISC',\n",
        "    'B-corporation':'B-ORG',\n",
        "    'I-corporation':'I-ORG',\n",
        "    'B-group':'B-MISC',\n",
        "    'I-group':'I-MISC',\n",
        "    'B-person':'B-PER',\n",
        "    'I-person':'I-PER',\n",
        "    'B-location':'B-LOC',\n",
        "    'I-location':'I-LOC',\n",
        "    'O':'O'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    for frame in data_wnut.values():\n",
        "        frame['tag'] = frame['tag'].map(tag_map)\n",
        "\n",
        "    wnut_sort = sorted(data_conll['train']['tag'].unique(), key=lambda x: (x[1:],x[0]))\n",
        "    conll_sort = sorted(data_wnut['train']['tag'].unique(), key=lambda x: (x[1:],x[0]))\n",
        "\n",
        "    assert all([i==j for i,j in zip(wnut_sort,conll_sort)]), 'Mismatched tags between CoNLL and WNUT data.'\n",
        "\n",
        "    log_print(f'Tag conversion successful. Tags: {str(wnut_sort)}')\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f'Failed: {e}')\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the self-annotated tweets\n",
        "path = '../data/' if 'src' in os.getcwd() else './data/'\n",
        "tweets = pd.read_csv(path+f'twitter/test_29_11_19.csv', index_col=['sentence_id','token_id']).fillna(value={'token': 'NA'})\n",
        "tweet_map = {\n",
        "    'B-EVENT':'B-MISC',\n",
        "    'I-EVENT':'I-MISC',\n",
        "    'B-NORP':'B-MISC',\n",
        "    'I-NORP':'I-MISC',\n",
        "    'B-WORK_OF_ART':'B-MISC',\n",
        "    'I-WORK_OF_ART':'I-MISC',\n",
        "    'B-PRODUCT':'B-MISC',\n",
        "    'I-PRODUCT':'I-MISC',\n",
        "    'B-FAC':'B-LOC',\n",
        "    'I-FAC':'I-LOC',\n",
        "    'B-PERSON':'B-PER',\n",
        "    'I-PERSON':'I-PER',\n",
        "    'B-GPE':'B-LOC',\n",
        "    'I-GPE':'I-LOC',\n",
        "    'B-LOC':'B-LOC',\n",
        "    'I-LOC':'I-LOC',\n",
        "    'B-ORG':'B-ORG',\n",
        "    'I-ORG':'I-ORG',\n",
        "    'O':'O'\n",
        "}\n",
        "\n",
        "tweets['tag'] = tweets['tag'].map(tweet_map)\n",
        "\n",
        "tweet_tags = sorted(tweets['tag'].unique(), key=lambda x: (x[1:],x[0]))\n",
        "log_print(f'Testing tweets ready. Tags: {str(tweet_tags)}')\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zDRRM_CPrgkN"
      },
      "outputs": [],
      "source": [
        "##Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p2CsZO-CuLpE"
      },
      "outputs": [],
      "source": [
        "Coming up with good features for our model to assign weights to is critical. We've used a number of orthographic features, prefixes, suffixes, POS tags and lemmas. The tag transition is also modelled under the hood by crfsuite. The context window is of radius 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sent2():\n",
        "    '''Takes an array of sentences and their tags'''\n",
        "\n",
        "    def __init__(self,attributes=None):\n",
        "        self.attrs = attributes\n",
        "\n",
        "    def __get_features(self, word, prefix):\n",
        "        '''get features dictionary for a single token'''\n",
        "\n",
        "        try:\n",
        "\n",
        "            features = {\n",
        "                f'{prefix}{attr}': word[attr] for attr in self.attrs\n",
        "            }\n",
        "\n",
        "            features.update({f'{prefix}bias': 1.0,\n",
        "            f'{prefix}prefix2': word['token'][:2],\n",
        "            f'{prefix}prefix3': word['token'][:2],\n",
        "            f'{prefix}suffix2': word['token'][-2:],\n",
        "            f'{prefix}suffix3': word['token'][-3:],\n",
        "            })\n",
        "\n",
        "        except TypeError:\n",
        "            key = 'BOS' if prefix == '-1' else 'EOS'\n",
        "            features = {key: True}\n",
        "        return features\n",
        "\n",
        "    def __word2features(self, sent, token_id):\n",
        "        '''get features dictionary over the context window'''\n",
        "\n",
        "        # get rows of context window\n",
        "        current = sent.iloc[token_id]\n",
        "        left = sent.iloc[token_id - 1] if token_id else None\n",
        "        try:\n",
        "            right = sent.iloc[token_id + 1]\n",
        "        except:\n",
        "            right = None\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        # add features from all tokens in context window\n",
        "        for row, prefix in zip((current, left, right), ('', '-1', '+1')):\n",
        "            features.update(self.__get_features(row, prefix))\n",
        "\n",
        "        features.pop('-1bias', None)\n",
        "        features.pop('+1bias', None)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def features(self, sent):\n",
        "        '''convert a sentence to a list of context window feature vectors'''\n",
        "        return [self.__word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "    def tokens(self, sent):\n",
        "        '''get the sequence of string tokens for a sentence'''\n",
        "        return ' '.join([row['token'] for _, row in sent.iterrows()])\n",
        "\n",
        "    def labels(self,sent):\n",
        "        '''get the sequence of target tags for a sentence'''\n",
        "        return [row['tag'] for _, row in sent.iterrows()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uxXwsghjvJgV"
      },
      "outputs": [],
      "source": [
        "Sample sentences from our dataframes and create the input and output pairs for training/testing. An input is a list of feature vectors, one for each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentences(frame, p=1, name='unknown'):\n",
        "    ''' Convert the multi-index frame into a DataFrame where rows contain a list of features vectors for each time step in sentence, and their targets.\n",
        "    If p is < 1, randomly sample the sentences (where p is obviously the proportion sampled).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        Sentences\n",
        "\n",
        "    '''  \n",
        "\n",
        "    frame = frame.drop(columns='cluster')\n",
        "\n",
        "    if p<1:\n",
        "        sample_idx = np.random.choice(range(len(frame.groupby(level='sentence_id'))),\n",
        "                                size=int(p*len(frame.groupby(level='sentence_id'))),\n",
        "                                replace=False)\n",
        "        frame = frame.loc[sample_idx,:]\n",
        "\n",
        "    att = list(frame.columns.values)\n",
        "    att.remove('tag')\n",
        "    # print('Using token features:\\n',att)\n",
        "\n",
        "    x = frame.drop(columns='tag'\n",
        "                    ).groupby(level='sentence_id'\n",
        "                    ).apply(lambda sent: Sent2(att).features(sent))\n",
        "    y = frame.groupby(level='sentence_id'\n",
        "                    ).apply(lambda sent: Sent2().labels(sent))\n",
        "    z = frame.groupby(level='sentence_id'\n",
        "                    ).apply(lambda sent: Sent2().tokens(sent))\n",
        "\n",
        "\n",
        "    # return pd.DataFrame(data={'x': x, 'y': y, 'tokens': z, 'dist': dist})\n",
        "    return pd.DataFrame(data={'x': x, 'y': y, 'z': z, 'dist': [name]*len(x)})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = 1\n",
        "\n",
        "try:\n",
        "    log_print('Getting sentences from multi-index dataframes...')\n",
        "    dists = {\n",
        "        # 'tweets': get_sentences(tweets, 1, 'tweet'),\n",
        "        'wnut_test': get_sentences(data_wnut['test'], p, 'wnut'),\n",
        "        'wnut_train': get_sentences(data_wnut['train'], p, 'wnut'),\n",
        "        # 'conll': get_sentences(data_conll['train'], p, 'conll')\n",
        "    }\n",
        "\n",
        "    # combine conll and wnut data and shuffle\n",
        "    # dists['mixed'] = pd.concat([train_conll,train_wnut]).reset_index(drop=True).sample(frac=1)\n",
        "except Exception as e:\n",
        "    logging.error(f'Failed: {e}')\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GajuLtYe4aCx"
      },
      "outputs": [],
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zl2-HRyWvdty"
      },
      "outputs": [],
      "source": [
        "The 'O' is not of interest to us; we are more interested in the other tags.\n",
        "Let's evaluate models based on a flat f1 score over the other tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = list(data_wnut['train']['tag'].unique())\n",
        "\n",
        "labels.remove('O')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "get_name = lambda dist_: f'./models/crf_{dt.now().strftime(\"%d_%m_%y\")}_{dist_}.pickle'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For grid searching for decent model regularisation parameters\n",
        "\n",
        "# dist = 'mixed'\n",
        "\n",
        "# model_name = get_name(dist)\n",
        "# train = dists[dist]\n",
        "\n",
        "# try:\n",
        "#     iters = 20\n",
        "#     cv = 3\n",
        "#     params_space = {\n",
        "#     'c1': scipy.stats.expon(scale=1),\n",
        "#     'c2': scipy.stats.expon(scale=0.02),\n",
        "#     }\n",
        "\n",
        "#     logging.info(f'Fitting {iters*cv} models...')\n",
        "#     rs = RandomizedSearchCV(CRF(all_possible_transitions=True), params_space, n_iter=iters, cv=cv, scoring=flat_f1, n_jobs=10, verbose=1)\n",
        "#     rs.fit(train['x'], train['y'])\n",
        "#     clf = rs.best_estimator_\n",
        "\n",
        "#     log_print(f'Fit successful. Best params {rs.best_params_}')\n",
        "# except Exception as e:\n",
        "#     logging.error(f'Failed: {e}')\n",
        "#     raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For fitting full model now (if grid search previously, full model in crf variable)\n",
        "\n",
        "dist = 'wnut_train'\n",
        "\n",
        "model_name = get_name(dist)\n",
        "train = dists[dist]\n",
        "best_params = {'c1':0.02, 'c2':0.005}\n",
        "\n",
        "try:\n",
        "    log_print(f'Fitting full model...')\n",
        "\n",
        "    clf = CRF(all_possible_transitions=True, **best_params)\n",
        "    clf.max_iterations = 100\n",
        "    clf.epsilon = 1e-6\n",
        "    clf.fit(train['x'], train['y'])\n",
        "\n",
        "    log_print(f'Fit successful.')\n",
        "except Exception as e:\n",
        "    log_print(f'Failed: {e}')\n",
        "    raise e\n",
        "finally:\n",
        "    try:\n",
        "        with open(model_name, 'wb') as f:\n",
        "            pickle.dump(clf,f)\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UIZh7ucB5FwC"
      },
      "outputs": [],
      "source": [
        "##Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ner_evaluation.ner_eval import collect_named_entities, compute_metrics, compute_precision_recall_wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def score(y_true, y_pred):\n",
        "    ''' Get counts for the metrics overall and per-entity '''\n",
        "\n",
        "    results = None\n",
        "    type_results = None\n",
        "    tags = ['MISC','PER','LOC','ORG']\n",
        "    metrics_ = ['correct', 'incorrect', 'partial', 'missed', 'spurious', 'possible', 'actual']\n",
        "    measures = ['ent_type', 'partial', 'exact', 'strict']\n",
        "\n",
        "    for y_true_, y_pred_ in zip(y_true,y_pred):\n",
        "        tmp_results, tmp_type_results = compute_metrics(collect_named_entities(y_true_), collect_named_entities(y_pred_), tags)\n",
        "\n",
        "        # aggregate overall results\n",
        "        if results is None:\n",
        "            results = tmp_results\n",
        "        else:\n",
        "            for eval_schema in measures:\n",
        "                for metric in metrics_:\n",
        "                    results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
        "\n",
        "        # aggregate results by entity type\n",
        "        if type_results is None:\n",
        "            type_results = tmp_type_results\n",
        "        else:\n",
        "            for e_type in tags:\n",
        "                for eval_schema in measures:\n",
        "                    for metric in metrics_:\n",
        "                        type_results[e_type][eval_schema][metric] += tmp_type_results[e_type][eval_schema][metric]\n",
        "\n",
        "    return results, type_results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = ['crf_25_11_19_conll','crf_25_11_19_wnut','crf_27_11_19_mixed']\n",
        "# models = ['crf_09_12_19_wnut_train']\n",
        "\n",
        "test = dists['tweets']\n",
        "\n",
        "for name in models:\n",
        "    model_name = f'./models/{name}.pickle'\n",
        "\n",
        "    # load the model\n",
        "    with open(model_name, 'rb') as f:\n",
        "        clf = pickle.load(f)\n",
        "        log_print(f'Loaded pre-trained model {model_name}.')\n",
        "\n",
        "    # regularisation weights\n",
        "    log_print(f'c1:{clf.c1}, c2:{clf.c2}')\n",
        "    \n",
        "    # predict on test set\n",
        "    y_pred = clf.predict(test['x'])\n",
        "\n",
        "    # flat f1 score\n",
        "    flat_f1_score = metrics.flat_f1_score(test['y'], y_pred,\n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "    log_print(f'{name} flat F1 score: '+str(flat_f1_score))\n",
        "\n",
        "    sorted_labels = sorted(labels, key = lambda x: (x[1:],x[0]))\n",
        "\n",
        "    # save predictions for wnut shared task\n",
        "    # with open('wnut_predictions.txt', 'wb') as f:\n",
        "    #     for sent_tokens, sent_trues, sent_preds in zip(data_wnut['test'].groupby(level='sentence_id'), test['y'], y_pred):\n",
        "    #         for token, gold, pred in zip(sent_tokens[1]['token'], sent_trues, sent_preds):\n",
        "    #             f.write(f'{token}\\t{gold}\\t{pred}\\n'.encode('utf-8'))\n",
        "    #         f.write('\\n'.encode('utf-8'))\n",
        "\n",
        "    # per-tag classification report\n",
        "    print(metrics.flat_classification_report(\n",
        "        test['y'], y_pred, labels=sorted_labels, digits=3\n",
        "    ))\n",
        "\n",
        "    # overall and per-entity type metric counts for each measure\n",
        "    overall_results, type_results = score(test['y'], y_pred)\n",
        "\n",
        "    # compute precision, recall, f1 for each measure\n",
        "    for tag, results in type_results.items():\n",
        "        res = compute_precision_recall_wrapper(results)\n",
        "        for measure in res.keys():\n",
        "            print(tag, measure)\n",
        "            print('f1:', round(2*((res[measure]['precision']*res[measure]['recall'])/(res[measure]['precision']+res[measure]['recall'])),3))\n",
        "        print('\\n')\n",
        "\n",
        "    results = compute_precision_recall_wrapper(overall_results)\n",
        "    for k, res in results.items():\n",
        "        print(k)\n",
        "        print('f1:', round(2*((res['precision']*res['recall'])/(res['precision']+res['recall'])),3))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transition and per-tag weights of each model\n",
        "\n",
        "# 'crf_25_11_19_conll', 'crf_25_11_19_wnut', 'crf_27_11_19_mixed'\n",
        "\n",
        "name = 'crf_27_11_19_mixed'\n",
        "test = dists['tweets']\n",
        "model_name = f'./models/{name}.pickle'\n",
        "\n",
        "with open(model_name, 'rb') as f:\n",
        "    clf = pickle.load(f)\n",
        "    log_print(f'Loaded pre-trained model {model_name}.')\n",
        "\n",
        "eli5.show_weights(clf, top=(5,5))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices\n",
        "\n",
        "model_name = f'./models/crf_27_11_19_mixed.pickle'\n",
        "\n",
        "with open(model_name,'rb') as f:\n",
        "    clf = pickle.load(f)\n",
        "\n",
        "y_pred = clf.predict(test['x'])\n",
        "\n",
        "flat_pred = [i for y in y_pred for i in y]\n",
        "flat_true = [i for y in test['y'] for i in y]\n",
        "\n",
        "conf = confusion_matrix(flat_true, flat_pred, labels=sorted_labels)\n",
        "\n",
        "conf_df = pd.DataFrame._from_arrays(conf, columns=sorted_labels, index=sorted_labels)\n",
        "\n",
        "ax = sns.heatmap(data=conf_df, cmap='Blues')\n",
        "ax.set(xlabel='Predicted', ylabel='True')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sent_to_flat_frame(X):\n",
        "    '''Takes list of lists (sentences) of dicts (feature vectors) of features.\n",
        "    Returns multiindex where each row is a context window feature vector + tag pair. This is required easy permutation later.\n",
        "    '''\n",
        "\n",
        "    flat_x = [(i,tok,sent) for sent,x in enumerate(X) for tok,i in enumerate(x)]\n",
        "    \n",
        "    features = [i for i,_,_ in flat_x]\n",
        "    token_id = [j for _,j,_ in flat_x]\n",
        "    sentence_id = [k for _,_,k in flat_x]\n",
        "\n",
        "\n",
        "    df = pd.DataFrame.from_records(features)\n",
        "    df['token_id'] = pd.Series(token_id)\n",
        "    df['sentence_id'] = pd.Series(sentence_id)\n",
        "    df.set_index(['sentence_id','token_id'], drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def importances(clf, scorer, X, y_true, labels, columns_to_shuffle=None, n_iters=3):\n",
        "    '''Shuffles the input on each feature column and reports the drop in performance.\n",
        "    '''\n",
        "\n",
        "    y_pred = clf.predict(X)\n",
        "    \n",
        "    base_score = scorer(y_true, y_pred, average='weighted', labels=labels)\n",
        "\n",
        "    reductions = {}\n",
        "\n",
        "    X_flat = sent_to_flat_frame(X)\n",
        "\n",
        "    if columns_to_shuffle is None:\n",
        "        columns_to_shuffle = X_flat.columns\n",
        "    \n",
        "    for c in columns_to_shuffle:\n",
        "        print(f'Shuffling on {c}')\n",
        "        reductions_c = []\n",
        "        for _ in range(n_iters):\n",
        "            X_flat_copy = X_flat.copy()\n",
        "\n",
        "            if isinstance(c,tuple):\n",
        "                for ci in c:\n",
        "                    X_flat_copy[ci] = np.random.permutation(X_flat[ci].values)\n",
        "            else:\n",
        "                X_flat_copy[c] = np.random.permutation(X_flat[c].values)\n",
        "\n",
        "            sents = X_flat_copy.groupby(level='sentence_id'\n",
        "                              ).apply(lambda sent: [{k:v for k,v in m.items() if pd.notnull(v)} for m in sent.to_dict('records')])\n",
        "            y_pred = clf.predict(sents)\n",
        "            c_score = scorer(y_true, y_pred, average='weighted', labels=labels)\n",
        "            reductions_c.append(-c_score + base_score)\n",
        "        reductions[str(c)] = reductions_c\n",
        "    \n",
        "    return (base_score,reductions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Permutation importance\n",
        "\n",
        "models = ['crf_25_11_19_conll','crf_25_11_19_wnut','crf_27_11_19_mixed']\n",
        "test = dists['tweets']\n",
        "fi_s = {}\n",
        "\n",
        "for model in models:\n",
        "\n",
        "    model_name = f'./models/{model}.pickle'\n",
        "\n",
        "    with open(model_name,'rb') as f:\n",
        "        clf = pickle.load(f)\n",
        "\n",
        "    log_print('Feature importances for '+model_name)\n",
        "\n",
        "    # columns = ['norm','lower']\n",
        "\n",
        "    base_score, reductions = importances(clf, metrics.flat_f1_score, test['x'], test['y'], labels, columns)\n",
        "\n",
        "    feature_importances = {k:{'mean':np.mean(v),'std':np.std(v), 'raw':v} for k,v in reductions.items()}\n",
        "    fi_sorted = sorted(feature_importances.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
        "    fi_s[model] = fi_sorted\n",
        "    \n",
        "    log_print(str(fi_sorted))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr\n",
        "X = tweets.loc[:,['norm','lower']]\n",
        "corr = spearmanr(X).correlation\n",
        "corr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# if os.name == 'nt':\n",
        "#     _x = [s['c1'] for s in rs.cv_results_['params']]\n",
        "#     _y = [s['c2'] for s in rs.cv_results_['params']]\n",
        "#     _c = [round(s,2) for s in rs.cv_results_['mean_test_score']]\n",
        "\n",
        "#     fig = plt.figure()\n",
        "#     fig.set_size_inches(12, 12)\n",
        "#     ax = plt.gca()\n",
        "#     ax.set_yscale('log')\n",
        "#     ax.set_xscale('log')\n",
        "#     ax.set_xlabel('C1')\n",
        "#     ax.set_ylabel('C2')\n",
        "#     ax.set_title(\"Randomized Hyperparameter Search Sampled CV Results (min={:0.3}, max={:0.3})\".format(\n",
        "#         min(_c), max(_c)\n",
        "#     ))\n",
        "\n",
        "#     sns.scatterplot(x=_x, y=_y, hue=_c, size=_c, ax=ax)\n",
        "\n",
        "#     # ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
        "\n",
        "#     print(f'Dark blue => {round(min(_c),3)}, dark red => {round(max(_c),3)}')\n",
        "\n",
        "#     logging.info('Evaluation successful.')"
      ]
    }
  ]
}